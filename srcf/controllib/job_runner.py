from __future__ import print_function

from contextlib import contextmanager
import logging
import os
import platform
import select
import signal
import sys
import traceback

import psycopg2.extensions

import sqlalchemy.orm
import sqlalchemy.sql.expression
import sqlalchemy.ext.compiler

from srcf import database
from srcf.controllib import jobs
import srcf.mail

from srcflib.email import EmailWrapper

from .postgresqlhandler import PostgreSQLHandler


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
logger.addHandler(PostgreSQLHandler({"host": "postgres.internal", "database": "sysadmins"}))

environment = jobs.get_environment()
runner_id_string = "{} {} {}".format(platform.node(), os.getpid(), environment)


RUNNER_LOCK_NUM = 0x3666309f80b06

pg_try_advisory_lock = sqlalchemy.sql.expression.func.pg_try_advisory_lock


class DatabaseLocked(Exception):
    pass


def email_error(job_id, message):
    subject = "[Control Panel] Job #{0} failed".format(job_id)
    srcf.mail.mail_sysadmins(subject, message)


class Listen(sqlalchemy.sql.expression.Executable,
             sqlalchemy.sql.expression.ClauseElement):
    def __init__(self, channel):
        self.channel = channel

    def __repr__(self):
        return "<Listen {}>".format(self.channel)


@sqlalchemy.ext.compiler.compiles(Listen, 'postgresql')
def compile_listen(element, compiler, **kw):
    return "LISTEN {}".format(element.channel)


@contextmanager
def exit_on_signal(logger):
    traps = {signal.SIGINT, signal.SIGTERM}
    trapped = None

    def handler(signum, frame):
        logger.info("Job in progress, deferring signal")
        nonlocal trapped
        trapped = signum
    originals = {}
    for sig in traps:
        originals[sig] = signal.signal(sig, handler)

    with EmailWrapper(prefix="[SRCF Control Panel]",
                      footer="This email was generated by the SRCF control panel.  If you need "
                             "help, you can send an email to soc-srcf-admin@lists.cam.ac.uk (or "
                             "reply to this one) to reach the sysadmins, or support@srcf.net for "
                             "the SRCF support team.  Make sure that you remove any passwords "
                             "from the email when replying."):
        yield

    for sig in traps:
        signal.signal(sig, originals[sig])
    if trapped:
        logger.info("Exiting due to previous signal {}".format(trapped))
        sys.exit()


def connect(environment):
    # By explicitly creating a connection, and not using the pool, the
    # connection should persist for the lifetime of the process (it won't
    # be closed after each .commit()), and therefore hold the session level
    # lock.
    conn = database.engine.connect()
    conn.detach()
    with conn.begin():
        lock_num = RUNNER_LOCK_NUM + int.from_bytes(environment.encode("utf-8"), "little")
        row, = conn.execute(pg_try_advisory_lock(lock_num))
        if not row[0]:
            raise DatabaseLocked

        conn.execute(Listen("jobs_insert"))

    sess = sqlalchemy.orm.Session(bind=conn)
    return conn, sess


def notifications(conn):
    # get the underlying psycopg2 conn
    conn = conn.connection

    while True:
        if conn.status != psycopg2.extensions.STATUS_READY:
            raise RuntimeError("Notifications cannot be recieved "
                               "if a transaction is left open")

        select.select([conn], [], [], 600)
        conn.poll()

        while conn.notifies:
            notify = conn.notifies.pop()
            yield int(notify.payload)


def queued_jobs(environment):
    """
    Yields a list of job ids.

    Liable to yield IDs twice.

    Does not leave a transaction open; moreover, requires that you commit
    your transactions (or notifications won't be received)
    """

    # Use a separate dedicated connection just for this, so that we
    # don't miss any notifications
    conn, sess = connect(environment)

    # We're using the advisory lock to protect us against other
    # job runners; not transactions (indeed, transactions would be
    # insufficient).

    # First, go through jobs added while the runner was offline

    existing = (
        sess.query(database.Job)
        .filter(database.Job.state == "queued",
                database.Job.environment == environment)
        .all()
    )

    existing_ids = [e.job_id for e in existing]
    del existing

    sess.commit()

    for i in existing_ids:
        yield i

    del existing_ids

    # And listen for new jobs...
    # It's possible that an ID was yielded twice; this is not a problem.

    for n in notifications(conn):
        yield n


def main(run_logger):
    sess = database.Session()
    database.queries.disable_automatic_session(and_use_this_one_instead=sess)
    for i in queued_jobs(environment):
        job = jobs.Job.find(id=i, sess=sess)
        if job.state != "queued":
            sess.rollback()
            continue
        if job.row.environment != environment:
            continue

        with exit_on_signal(run_logger):

            job.logger = logger

            job.log("Running (host: {0})".format(runner_id_string), "started", logging.INFO, str(job))
            job.set_state("running", "Running (host: {0})".format(runner_id_string))
            sess.add(job.row)
            sess.commit()

            run_state = "failed"
            run_message = None

            try:
                run_message = job.run(sess=sess) or "Completed"
                run_state = "done"
                job.log(run_message, "done", logging.INFO)

            except jobs.JobFailed as e:
                run_message = e.message or "Aborted"
                job.log(run_message, "failed", logging.WARNING, e.raw)

                raw = e.raw
                if isinstance(raw, bytes):
                    raw = raw.decode("utf-8")

                email_error(i, "{0}\n\n{1}".format(run_message, raw) if raw else run_message)

            except Exception:
                job.log("Unhandled exception", "failed", logging.ERROR, traceback.format_exc(), exc_info=1)

                # rollback
                sess.rollback()
                job = jobs.Job.find(id=i, sess=sess)

                exc = traceback.format_exception_only(*sys.exc_info()[:2])[0].strip()
                run_message = exc

                email_error(i, run_message)

            job.set_state(run_state, run_message)
            sess.add(job.row)
            sess.commit()


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s %(levelname)s %(name)s [%(task)s]: %(message)s")
    run_logger = logging.LoggerAdapter(logger, {"task": "Runner"})
    run_logger.info("Starting job runner (host: {0})".format(runner_id_string))
    try:
        main(run_logger)
    except (KeyboardInterrupt, SystemExit):
        pass
    except Exception:
        run_logger.exception("Unhandled exception")
        raise
